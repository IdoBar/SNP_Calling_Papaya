---
title: "Papaya SNP calling from WGRS pipeline"
author: "Leela Manoharan and Ido Bar"
date: "`r format(Sys.Date(), '%d %B %Y')`"
always_allow_html: yes
output: 

    bookdown::html_document2:
      includes:
       in_header: style/header.html
       after_body: style/external-links-js.html
      df_print: paged
      theme: 
        version: 5
        bootswatch: simplex #sandstone #zephyr # yeti # united
        # primary: "#6CC3D4"
      highlight: tango # pygments
      css: "style/style.css"
      toc: true
      pandoc_args: ["--shift-heading-level-by=-1"]
      toc_float: true
      toc_depth: 3
  #    highlight: pygments
      number_sections: true
      code_folding: hide
#      keep_md: true
bibliography: ["style/papaya_gwas_refs.json"]
csl: style/springer-basic-improved-author-date-with-italic-et-al-period.csl
---

```{r setup, include=FALSE}
# install.packages("pak")
my_packages <- c('tidyverse', 'janitor', 'scales', 'bookdown', 'rmdformats', 'devtools', 'readxl',
                 "DT", "formattable", "reactablefmtr", "gt")
# pak::pak(my_packages, ask = FALSE)
devtools::source_gist("7f63547158ecdbacf31b54a58af0d1cc", filename = "util.R")
# install Rstudio addins
# pak::pak(c("paleolimbot/rbbt", "milesmcbain/datapasta", "ThinkR-open/remedy", "styler", "hippie", "gadenbuie/ermoji",
#            "haozhu233/giphyr", "gadenbuie/regexplain","addinslist"), ask = FALSE)
knitr::opts_chunk$set(list(echo = TRUE, eval=FALSE, message=FALSE))
# knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)


pacman::p_load(char = basename(my_packages), install = FALSE, update = FALSE)
rbbt::bbt_write_bib('style/papaya_gwas_refs.json', overwrite = TRUE)

customGreen0 = "#DeF7E9"

customGreen = "#71CA97"

customRed = "#ff7f7f"
```

## Experimental Design

Fresh leaf samples were collected from XX genotypes of papaya (*Carica papaya*) from Rocky Top farm in North Queensland in 2023-24 and were stored in liquid Nitrogen and were stored in -80°C until DNA extraction. DNA was extracted from the leaf samples using the Omega Biotek E-Z 96 Plant DNA extraction kit. The extracted DNA was shipped in dry ice to the State Agricultural Biotechnology Centre ([SABC](https://www.murdoch.edu.au/research/sabc)) at Murdoch University (led by Prof. Rajeev Varshney). DNA libraries were prepared for whole-genome resequencing (WGRS) on an MGI DNBSEQ-T7, producing 150 bp paired-end reads.


### Samples used in this study

```{r sample-metadata, eval = TRUE}
sabc_metadata <- read_tsv('data/papaya_wgrs_Murdoch_files.txt') 
sabc_meta_sum <- sabc_metadata %>% 
  mutate(genotype=sub("\\.E2.+", "", filename)) %>% 
  group_by(genotype) %>% 
  summarise(total_size_mb = sum(size)/1e6, file_num=n())
write_xlsx(sabc_meta_sum, "data/papaya_WGRS_SRA_metadata.xlsx", sheet = "SABC_WGRS_sum",
           overwritesheet=TRUE)

sra_sheets <- excel_sheets("data/Cpapaya_All_SRA_WGS_metadata.xlsx") %>% 
  grep("sra_wgs$", ., value = TRUE)
sra_seq_data <- map(sra_sheets, .f = ~read_excel("data/Cpapaya_All_SRA_WGS_metadata.xlsx", sheet =.x) %>% 
  clean_names() %>%
    mutate(sample_title = gsub("\\s+", "", sample_name))) %>% 
  list_rbind() %>% 
  mutate(platform = ifelse(is.na(platform), 
                           sub("\\s.+$","", instrument), platform),
         model = ifelse(is.na(model), 
                           sub("^[^ ]+\\s+","", instrument), model))



sra_sample_data <- read_excel("data/Cpapaya_PRJNA727683_sra_wgs.xlsx", sheet = 'Accession_table') %>% 
  clean_names() %>% 
  mutate(cultivar_name=sub("‘*(.+)'", "\\1", cultivar_name),
         name = gsub("\\s+", "", name))



# sra_seq_data$sample_title[!sra_seq_data$sample_title %in%  sra_sample_data$name]

sra_combined <- sra_seq_data  %>% 
  select(name=sample_title, total_size_mb, sra_accession) %>% 
  left_join(sra_sample_data)

#sra_combined %>% filter(is.na(sex))
# sra_combined %>% 
#   mutate(sra_accession = glue::glue("[{sra_accession}](https://www.ncbi.nlm.nih.gov/sra/{sra_accession})"),
#         sra_accession = map(sra_accession, gt::md),
#         total_size_mb = comma(total_size_mb)) %>%
#   clean_names(case="title", abbreviations="SRA") %>% 
#     gt()


# sra_combined %>% 
#   mutate(sra_accession = glue::glue("<a href=https://www.ncbi.nlm.nih.gov/sra/{sra_accession}>{sra_accession}</a>")
#         # total_size_mb = comma(total_size_mb), 
#          # sra_accession = map(sra_accession, gt::md)
#         ) %>%
#         clean_names(case="title", abbreviations="SRA") %>%
# formattable(.,
#             list(
#   `Total Size Mb` = color_bar(customGreen)
# ))

# sra_combined %>% 
#   mutate(sra_accession = glue::glue("[{sra_accession}](https://www.ncbi.nlm.nih.gov/sra/{sra_accession})"),
#         sra_accession = map(sra_accession, gt::md)) %>% 
#         # total_size_mb = comma(total_size_mb)) %>%
#   clean_names(case="title", abbreviations="SRA") %>% 
# reactable(.,
#           columns = list(
#             "Total Size Mb" = colDef(
#             cell = data_bars(.,
#                      fill_color = viridis(5),
#                      background = "lightgrey",
#                      text_position = "inside-end",
#                      max_value = 15000)
#             )
#           ))

```

```{r sra-table, eval=TRUE, echo=FALSE}
# paste0('<a  target=_blank href=', path, '>', file,'</a>' )
sra_combined %>% 
  mutate(sra_accession = glue::glue("<a href=https://www.ncbi.nlm.nih.gov/sra/{sra_accession}>{sra_accession}</a>")
        # total_size_mb = comma(total_size_mb), 
         # sra_accession = map(sra_accession, gt::md)
        ) %>%
        clean_names(case="title", abbreviations="SRA") %>%
    datatable(., extensions = c('ColReorder','Responsive'), options = list(colReorder = TRUE),
              # colnames = snakecase::to_title_case(names(.)), 
              # caption = 'Table 1: This is a simple caption for the table.',
              escape = FALSE) %>% formatRound('Total Size Mb') %>% 
  formatStyle(
    'Total Size Mb',
    background = styleColorBar(sra_combined$total_size_mb, 'lightblue'),
    backgroundSize = '100% 90%',
    backgroundRepeat = 'no-repeat',
    backgroundPosition = 'center'
  )

```

## Aims

1. Assess the genetic diversity of papaya genotypes in the Australian collection  
  - Align the sequencing reads to a reference genome (Solo Sunset or Holland_5)  
  - Call variants (SNPs)  
  - Infer phylogeny and diversity
2. Identify variants associated with productivity and fruit quality traits   
  - Assemble a pangenome (or use the one from Murdoch)  
  - Align WGRS reads to the pangenome using `vg-giraffe`  
  - Call variants  
  - Perform genome wide association study (GWAS) for a range of traits
  

## Methods

### Overview of Analysis Pipeline

0.  Obtain sequencing data from public repositories (NCBI SRA) or from SABC
1.  Data pre-processing:  
    a.  Quality check  
    b.  Adaptor trimming  
    c.  Post-trim quality check(?)  
2.  Mapping reads to a reference genome  
3.  Reads deduplication and read group addition  
4.  Variant calling and filtration  
5.  Population genetics analysis (diversity and phylogeny)  

Sequencing data processing, mapping and variant calling were performed on the *QCIF Bunya* (using Slurm scheduler, see [documentation](https://github.com/UQ-RCC/hpc-docs/tree/main)) and in [GalaxyAU](https://usegalaxy.org.au). 


### Setup compute environment

Install needed software in a `conda` environment on the HPC cluster (we will install a [Miniforge distribution](https://github.com/conda-forge/miniforge), which has `mamba` already installed - see [mamba docs](https://mamba.readthedocs.io/en/latest/installation/mamba-installation.html)).

```{bash setup-conda}
alias start_interactive_job='salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=10 --mem=50G --job-name=interactive-job --time=05:00:00 --partition=general --account=a_agri_genomics srun --export=ALL,PATH,TERM,HOME,LANG --pty /bin/bash -l'
start_interactive_job

mkdir -p ~/bin

# Prepare a general array Slurm script
echo '#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --output=%x.%A.%a.log'"
#SBATCH --account=a_agri_genomics
#SBATCH --partition=general

set -Eeo pipefail
source ~/.bashrc
conda activate \$CONDA_NAME
cd \$SLURM_SUBMIT_DIR
gawk -v ARRAY_IND=\$SLURM_ARRAY_TASK_ID 'NR==ARRAY_IND' \$CMDS_FILE | bash" > ~/bin/array.slurm

echo '#!/bin/bash --login
#SBATCH --nodes=1
#SBATCH --ntasks=1
#SBATCH --output=%x.%j.log'"
#SBATCH --account=a_agri_genomics
#SBATCH --partition=general

set -Eeo pipefail
source ~/.bashrc
conda activate \$CONDA_NAME
cd \$SLURM_SUBMIT_DIR
bash \$CMDS_FILE" > ~/bin/serial_jobs_run.slurm

# Prepare a parallel jobs Slurm script
echo '
cat $CMDS_FILE | parallel' | cat <(head -n -1  ~/bin/serial_jobs_run.slurm) - > ~/bin/parallel_jobs_run.slurm


# download and install miniforge conda and environment
JOBNAME="install-conda"
NCORES=6
MEM=32
WALLTIME=5:00:00
CONDA_NAME="genomics"
echo "wget \"https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-$(uname)-$(uname -m).sh\"
bash Miniforge3-$(uname)-$(uname -m).sh -b
# accept defaults and let conda initialise
# initialise conda
source ~/.bashrc
# add channels and set priorities
conda config --add channels conda-forge
conda config --append channels bioconda
conda config --set auto_stack 1

# install extra packages to the base environment
mamba install -n base libgcc gnutls libuuid readline cmake git tmux libgfortran parallel mamba gawk pigz rename genozip autoconf sshpass gh numpy scipy datamash aria2
# install snippy (need to fix internet connection to gowonda2 - use patched netcheck in ~/bin)
# source ~/.proxy

mamba create -n $CONDA_NAME sra-tools bcbio-gff libgd xorg-libxpm \
                libpng libjpeg-turbo jpeg snpsift biobambam bwa-mem2 sambamba \
                libtiff qualimap multiqc bbmap fastp freebayes bedops \
                entrez-direct parallel-fastq-dump mosdepth bamtools perl-text-csv 
# Clean extra space
# conda update -n base conda
conda clean -y --all" > $JOBNAME.cmds

# submit to the cluster
JOB_ID=$(sbatch  --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$JOBNAME.cmds,CONDA_NAME=base ~/bin/serial_jobs_run.slurm | cut -f 4 -d " ")
```

### Data preparation

Create a folder for our processing (`$RUN_DIR`) and prepare (create accessory index files) the reference genome (Holland_5 - PAPHI003, Haplotype 2).

```{bash prep-genomes}
# download the SunUp genome from SharePoint
rclone copy -P  Papaya_genomics:Genomic_Resources/SunUp_SunSet_genomes_Nature_paper/SunUp /scratch/project/adna/Papaya/SunUp_reference_genome
# start an interactive job on Bunya
alias start_interactive_job='salloc --nodes=1 --ntasks-per-node=1 --cpus-per-task=10 --mem=50G --job-name=interactive --time=05:00:00 --partition=general --account=a_agri_genomics srun --export=PATH,TERM,HOME,LANG --pty /bin/bash -l'
start_interactive_job
WORK_DIR="/scratch/project/adna/leela"

CONDA_NAME="genomics"
conda activate $CONDA_NAME
# Prepare the commands
# REFERENCE="/scratch/project/adna/Papaya/SunUp_reference_genome/GWHBFSC00000000" # SunUp genome on Bunya HPC
REFERENCE="/scratch/project/adna/Papaya/Murdoch_sequencing/papaya_ref_grade/PAPHI003/hap2/PAPHI003_hap2" # Holland_5 genome from Murdoch on Bunya HPC
cd $WORK_DIR
# pigz -cd  $REF_DIR/ArME14.fasta.gz > $GENOME.fa
# pigz -cd  $REFERENCE.gff.gz > $REFERENCE.gff3
gff2bed < $REFERENCE.gff > $REFERENCE.bed
# cat $REFERENCE.fasta.gz | bgzip -c -I $REFERENCE.fasta.gz.gzi -@ $SLURM_CPUS_PER_TASK > $REFERENCE.fasta.gz && samtools faidx $REFERENCE.fasta.gz && bwa-mem2 index $REFERENCE.fasta.gz 
# samtools faidx $REFERENCE.fasta && bwa-mem2 index $REFERENCE.fasta 

cat $REFERENCE.fasta | bgzip -c > $REFERENCE.fasta.gz
samtools faidx --fai-idx $REFERENCE.fasta.fai --gzi-idx $REFERENCE.fasta.gz.gzi $REFERENCE.fasta.gz
bwa-mem2 index $REFERENCE.fasta 

```

Download the raw papaya whole-genome resequencing files (`.fastq.gz`) from NCBI SRA (WGS libraries from [PRJNA727683](https://www.ncbi.nlm.nih.gov/bioproject/PRJNA727683/),   [PRJNA693144](https://www.ncbi.nlm.nih.gov/bioproject/PRJNA693144/), [PRJNA970517](https://www.ncbi.nlm.nih.gov/bioproject/PRJNA970517/) and [PRJNA271489](https://www.ncbi.nlm.nih.gov/bioproject/PRJNA271489/)).
Reference genomes of cultivars Solo SunUp (GMO for PRSV resistance, accession [GWHBFSC00000000](https://ngdc.cncb.ac.cn/gwh/Assembly/23161/show), corresponding to NCBI accession [GCA_021527605.1](https://www.ncbi.nlm.nih.gov/datasets/genome/GCA_021527605.1/), but including gene annotation) and Sunset (accession [GWHBFSD00000000](https://ngdc.cncb.ac.cn/gwh/Assembly/23162/show), corresponding to NCBI [GCA_022788785.1](https://www.ncbi.nlm.nih.gov/datasets/genome/GCA_022788785.1/), but including gene annotation; @yueSunUpSunsetGenomes2022) were downloaded from the [Genome Warehouse](https://ngdc.cncb.ac.cn/gwh).

```{bash retrieve-sra-info}
# create a working directory
WORK_DIR="/scratch/project/adna/leela"
mkdir -p $WORK_DIR
cd $WORK_DIR
# download Papaya sequencing files from SRA
JOBNAME="download-papaya-SRA"
NCORES=6
MEM=32
WALLTIME=5:00:00
CONDA_NAME="genomics"
conda activate $CONDA_NAME
# get WGS information from the SRA 
parallel --dry-run "esearch -db sra -query {} | ~/adna/tools/edirect/efetch -format runinfo  > Cpapaya_{}_SRA_runinfo.csv " ::: PRJNA727683 PRJNA693144 PRJNA970517 PRJNA271489
# save just the headers
head -n1 Cpapaya_PRJNA271489_SRA_runinfo.csv > Cpapaya_All_SRA_WGS_runinfo.csv


# combining SRAs from all four projects
cat Cpapaya_PRJNA*_SRA_runinfo.csv | gawk -F "," '$13=="WGS"' >> Cpapaya_All_SRA_WGS_runinfo.csv

#combining all SRAs and sample names from the bioproject and check for duplicates


# cat Cpapaya_PRJNA727683_SRA_WGS_runinfo.csv | cut -f 1 -d "," | parallel --dry-run "prefetch {};  parallel-fastq-dump --split-3 -t \$SLURM_CPUS_PER_TASK --sra-id {}/{}.sra --gzip -O {}" > $JOBNAME.cmds

# submit to the cluster
# ARRAY_ID=$(sbatch -a 1-$(cat $JOBNAME.cmds | wc -l) --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/array.slurm | cut -f 4 -d " ")
```

Process SRA metadata table to remove duplicates.

```{r sra-data-table}

sra_info <- read_csv("data/Cpapaya_All_SRA_WGS_runinfo.csv")
sra_info_filtered <- sra_info %>% filter(!grepl("454", Platform)) %>% 
  mutate(SampleName = gsub("\\s+", "", SampleName)) %>% 
  distinct(SampleName, spots, bases, .keep_all = TRUE) %>% 
  write_csv("data/Cpapaya_All_SRA_WGS_runinfo_filtered.csv")

```

### Mapping to the reference genome{#bwamem-align}

Reads were mapped to the *C. papaya* SunUp reference genome assembled and annotated by the Fujian Agriculture and Forestry University [@yueSunUpSunsetGenomes2022], using bwa-mem2 v2.2.1 [@vasimuddinEfficientArchitectureAwareAcceleration2019]. The alignment files were then coordinate-sorted and PCR duplicates were marked using the `bamsormadup` command from BioBamBam2 v2.0.183 [@tischlerBiobambamToolsRead2014].\
Mapping quality and coverage were assessed with Qualimap v.2.2.2-dev [@okonechnikovQualimapAdvancedMultisample2016] and Mosdepth v0.3.10 [@pedersenMosdepthQuickCoverage2018] and were consolidated along with quality-trimming measures into a single, interactive report for each batch using MultiQC v1.21 [@ewelsMultiQCSummarizeAnalysis2016]. 
<!-- Samples with less than 20% mapping and x15 coverage were removed from the rest of the analysis (see details in the [MultiQC report](raw_data/QC_11_10_2021_multiqc_report.html)). -->

```{bash process-align-sra-reads}
# setup workspace
CONDA_NAME="genomics"
WORK_DIR="/scratch/project/adna/leela"
# save the run date
#DATE=$(date +%d_%m_%Y)
#RUN_DIR="$WORK_DIR/FB_SNP_calling_${DATE}"
RUN_DIR="$WORK_DIR/FB_SNP_calling_10_10_2025"
# REFERENCE="/scratch/project/adna/Papaya/SunUp_reference_genome/GWHBFSC00000000"
REFERENCE="/scratch/project/adna/Papaya/Murdoch_sequencing/papaya_ref_grade/PAPHI003/hap2/PAPHI003_hap2"
# BAM_DIR="$RUN_DIR/aligned_reads"
mkdir -p  $RUN_DIR && cd $RUN_DIR
NCORES=12
MEM=96
WALLTIME="5:00:00"
JOBNAME="SRA-fastp-bwa"

# fields in SRA csv file: ID:{1},SM:{30},PL:{19},PM:{20}
cat $WORK_DIR/Cpapaya_All_SRA_WGS_runinfo_filtered.csv | parallel --csv --dry-run " prefetch {1} && cd {1} && parallel-fastq-dump --split-3 -t \$SLURM_CPUS_PER_TASK --sra-id {1}.sra --gzip && fastp -i {1}_1.fastq.gz -I {1}_2.fastq.gz --detect_adapter_for_pe -c -l 30 -p -w \$SLURM_CPUS_PER_TASK -z 7 -o {1}_R1.trimmed.fastq.gz -O {1}_R2.trimmed.fastq.gz -j {1}.fastp.json -h {1}.fastp.html && bwa-mem2 mem -R \"@RG\tID:{1}\tSM:{30}.{1}\tLB:{30}.{1}\tPL:{19}\tPM:{20}\" -t \$[SLURM_CPUS_PER_TASK - 2] $REFERENCE.fasta.gz {1}_R1.trimmed.fastq.gz {1}_R2.trimmed.fastq.gz | bamsormadup tmpfile=\$TMPDIR/bamsormadup_\$(hostname)_\$SLURM_ARRAY_JOB_ID inputformat=sam threads=\$[SLURM_CPUS_PER_TASK - 2] indexfilename={1}.dedup.rg.csorted.bam.bai > {1}.dedup.rg.csorted.bam && mkdir -p {1}_bamqc &&  mosdepth -t \$SLURM_CPUS_PER_TASK -x -n {1}_bamqc/{1} {1}.dedup.rg.csorted.bam; unset DISPLAY; qualimap bamqc -bam {1}.dedup.rg.csorted.bam --java-mem-size=32G -c -gff $REFERENCE.bed -outdir {1}_bamqc && rm {1}.sra {1}_[12].fastq.gz"  > $RUN_DIR/$JOBNAME.cmds

# check how many commands were written
wc -l $RUN_DIR/$JOBNAME.cmds
# look at the first command and make sure it looks correct
head -n1 $RUN_DIR/$JOBNAME.cmds


# submit to the cluster
ARRAY_ID=$(sbatch -a 1-$(cat $RUN_DIR/$JOBNAME.cmds | wc -l) --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/array.slurm | cut -f 4 -d " ")

# find if any jobs failed
FAILED_TASKS=$(sacct -n -X -j $ARRAY_ID -o state%20,jobid%20 | grep -v COMPLETED | gawk '{print $2}' | cut -d"_" -f2 | paste -sd,)
# Change job directives (if needed)
# MEM=64
WALLTIME="10:00:00"
# retry failed jobs
sbatch -a $FAILED_TASKS --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/array.slurm 


```

Just align the pre-processed reads 

```{bash align-sra-reads}
# setup workspace
CONDA_NAME="genomics" 
WORK_DIR="/scratch/project/adna/leela"
# save the run date
#DATE=$(date +%d_%m_%Y)
#RUN_DIR="$WORK_DIR/FB_SNP_calling_${DATE}"
RUN_DIR="$WORK_DIR/FB_SNP_calling_10_10_2025"
# REFERENCE="/scratch/project/adna/Papaya/SunUp_reference_genome/GWHBFSC00000000"
REFERENCE="/scratch/project/adna/Papaya/Murdoch_sequencing/papaya_ref_grade/PAPHI003/hap2/PAPHI003_hap2"
# BAM_DIR="$RUN_DIR/aligned_reads"
mkdir -p  $RUN_DIR && cd $RUN_DIR
NCORES=12
MEM=96
WALLTIME="8:00:00"
JOBNAME="SRA-bwa"

# fields in SRA csv file: ID:{1},SM:{30},PL:{19},PM:{20}
cat $WORK_DIR/Cpapaya_All_SRA_WGS_runinfo_filtered.csv | parallel --header : --csv --dry-run "cd {1} && bwa-mem2 mem -R \"@RG\tID:{1}\tSM:{30}.{1}\tLB:{30}.{1}\tPL:{19}\tPM:{20}\" -t \$[SLURM_CPUS_PER_TASK - 2] $REFERENCE.fasta {1}_R1.trimmed.fastq.gz {1}_R2.trimmed.fastq.gz | bamsormadup tmpfile=\$TMPDIR/bamsormadup_\$(hostname)_\$SLURM_ARRAY_JOB_ID inputformat=sam threads=\$[SLURM_CPUS_PER_TASK - 2] indexfilename={1}.dedup.rg.csorted.bam.bai > {1}.dedup.rg.csorted.bam && mkdir -p {1}_bamqc &&  mosdepth -t \$SLURM_CPUS_PER_TASK -x -n {1}_bamqc/{1} {1}.dedup.rg.csorted.bam; unset DISPLAY; qualimap bamqc -bam {1}.dedup.rg.csorted.bam --java-mem-size=32G -c -gff $REFERENCE.bed -outdir {1}_bamqc"  > $RUN_DIR/$JOBNAME.cmds

# check how many commands were written
wc -l $RUN_DIR/$JOBNAME.cmds
# look at the first command and make sure it looks correct
head -n1 $RUN_DIR/$JOBNAME.cmds
# submit to the cluster
ARRAY_ID=$(sbatch -a 1-$(cat $RUN_DIR/$JOBNAME.cmds | wc -l) --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/array.slurm | cut -f 4 -d " ")

# find if any jobs failed
FAILED_TASKS=$(sacct -n -X -j $ARRAY_ID -o state%20,jobid%20 | grep -v COMPLETED | gawk '{print $2}' | cut -d"_" -f2 | paste -sd,)
# Change job directives (if needed)
# MEM=64
WALLTIME="10:00:00"
# retry failed jobs
sbatch -a $FAILED_TASKS --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/array.slurm 


```

Separately align the WGRS reads generated on an MGI DNBSEQ-T7 at SABC (Murdoch University).

```{bash process-align-wgrs-reads}
# setup workspace
CONDA_NAME="genomics" 
WORK_DIR="/scratch/project/adna/leela"
# save the run date
#DATE=$(date +%d_%m_%Y)
#RUN_DIR="$WORK_DIR/FB_SNP_calling_${DATE}"
RUN_DIR="$WORK_DIR/FB_SNP_calling_10_10_2025"
# REFERENCE="/scratch/project/adna/Papaya/SunUp_reference_genome/GWHBFSC00000000"
REFERENCE="/scratch/project/adna/Papaya/Murdoch_sequencing/papaya_ref_grade/PAPHI003/hap2/PAPHI003_hap2"
FQ_DIR="/scratch/project/adna/Papaya/Murdoch_sequencing/papaya_wgrs"
BAM_DIR="$RUN_DIR/aligned_reads"
mkdir -p $FQ_DIR/trimmed_reads/QC $RUN_DIR && cd $RUN_DIR
NCORES=12
MEM=96
WALLTIME="8:00:00"
RGPM="DNBseq_T7"
RGPL="MGI"
# RGPU="E250038400"
RGCN="MU_SABC"

JOBNAME="wgrs-fastp-bwa"
IGNORE_FQ="Eksotika_PAPHI_88|Holland_6_PAPHI_87"
# reanme sample names so they don't include a dot.

rename -v 's/6-4.1./6-4-1-/g'  $FQ_DIR/C1-6-4.1.*
rename -v 's/6.4.1./6-4-1-/g'  $FQ_DIR/C1-6.4.1.*

find $FQ_DIR/*.R1.fq.gz | egrep -v $IGNORE_FQ | parallel -k --dry-run --rpl "{file2} s:.R1:.R2:; uq()" --rpl "{sample} s:.+\/([^\.]+)\.(E\d+).+.fq.gz:\1:" --rpl "{rgpu} s:.+\/([^\.]+)\.(E\d+).+.fq.gz:\2:"  "fastp -i {} -I {file2} --detect_adapter_for_pe -c -l 30 -p -w \$SLURM_CPUS_PER_TASK -z 7 -o $FQ_DIR/trimmed_reads/{sample}.{rgpu}.R1.trimmed.fastq.gz -O $FQ_DIR/trimmed_reads/{sample}.{rgpu}.R2.trimmed.fastq.gz -j $FQ_DIR/trimmed_reads/QC/{sample}.{rgpu}.fastp.json -h $FQ_DIR/trimmed_reads/QC/{sample}.{rgpu}.fastp.html; mkdir -p $BAM_DIR/{sample}/{sample}_bamqc; bwa-mem2 mem -R \"@RG\tID:{sample}.{rgpu}\tSM:{sample}\tLB:{sample}.{rgpu}\tPU:{rgpu}\tPL:$RGPL\tPM:$RGPM\tCN:$RGCN\" -t \$[SLURM_CPUS_PER_TASK - 2] $REFERENCE.fasta $FQ_DIR/trimmed_reads/{sample}.{rgpu}.R1.trimmed.fastq.gz $FQ_DIR/trimmed_reads/{sample}.{rgpu}.R2.trimmed.fastq.gz | bamsormadup tmpfile=\$TMPDIR/bamsormadup_\$(hostname)_\$SLURM_ARRAY_JOB_ID inputformat=sam threads=\$[SLURM_CPUS_PER_TASK - 2] indexfilename=$BAM_DIR/{sample}/{sample}.{rgpu}.dedup.rg.csorted.bam.bai > $BAM_DIR/{sample}/{sample}.{rgpu}.dedup.rg.csorted.bam ; mosdepth -t \$SLURM_CPUS_PER_TASK -x -n $BAM_DIR/{sample}/{sample}_bamqc/{sample}.{rgpu} {sample}.{rgpu}.dedup.rg.csorted.bam; unset DISPLAY; qualimap bamqc -bam $BAM_DIR/{sample}/{sample}.{rgpu}.dedup.rg.csorted.bam --java-mem-size=32G -c -gff $REFERENCE.bed -outdir $BAM_DIR/{sample}/{sample}_bamqc/{sample}.{rgpu}"  > $RUN_DIR/$JOBNAME.cmds

# check how many commands were written
wc -l $RUN_DIR/$JOBNAME.cmds
# look at the first command and make sure it looks correct
head -n1 $RUN_DIR/$JOBNAME.cmds
# submit to the cluster
ARRAY_ID=$(sbatch -a 1-$(cat $RUN_DIR/$JOBNAME.cmds | wc -l) --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/array.slurm | cut -f 4 -d " ")

# find if any jobs failed
FAILED_TASKS=$(sacct -n -X -j $ARRAY_ID -o state%20,jobid%20 | grep "FAILED" | gawk '{print $2}' | cut -d"_" -f2 | paste -sd,)
# Change job directives (if needed)
# MEM=64
WALLTIME="10:00:00"
# rerun missing/failed jobs
JOBNAME="wgrs-fastp-bwa2"
egrep "Papaya_19|Papaya_24" $RUN_DIR/wgrs-fastp-bwa.cmds > $RUN_DIR/$JOBNAME.cmds

ARRAY_ID=$(sbatch -a 1-$(cat $RUN_DIR/$JOBNAME.cmds | wc -l) --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/array.slurm | cut -f 4 -d " ")
```


Duplicated `BAM` files (of the same sample that was sequenced more than once) were merged after alignment with samtools v1.21 [@danecekTwelveYearsSAMtools2021; @liSequenceAlignmentMap2009a] or bamtools v2.5.2.
_!Need to check what happened to read groups of duplicated samples!_

```{bash merge-bams}
# setup workspace
CONDA_NAME="genomics" 
WORK_DIR="/scratch/project/adna/leela"
# save the run date
#DATE=$(date +%d_%m_%Y)
#RUN_DIR="$WORK_DIR/FB_SNP_calling_${DATE}"
RUN_DIR="$WORK_DIR/FB_SNP_calling_02_09_2025"
BAM_DIR="$RUN_DIR/aligned_reads"

NCORES=8
MEM=48
WALLTIME="2:00:00"
JOBNAME="wgrs-merge-bams"

# find $BAM_DIR -maxdepth 1 -mindepth 1 -type d
# DUPS=$(find $BAM_DIR -name "*.E*.dedup.rg.csorted.bam" | gawk -F "/" '{print $2}' | sort | uniq -c |  gawk  '$1>1{printf"%s%s",c,$2;c="|"}')

# Find duplicate BAMs to merge
find $BAM_DIR -name "*.E*.dedup.rg.csorted.bam" | xargs dirname | sort | uniq -d | parallel --dry-run "ls -1 {}/{/}.E*.dedup.rg.csorted.bam > {}/{/}.bamlist; bamtools merge -list {}/{/}.bamlist -out {}/{/}.merged.bam; bamtools index -in {}/{/}.merged.bam; rm {}/{/}.E*.dedup.rg.csorted.bam*" > $RUN_DIR/$JOBNAME.cmds


# Unnecessary complicated
#find aligned_reads/ -name "*.E*.dedup.rg.csorted.bam" | gawk -F "/" '{print $0, $2}' | gawk '$2 in first{print first[$2] $0; first[$2]=""; next} {first[$2]=$0 ORS}' | gawk '{print $1}' | xargs dirname | sort | uniq 


# submit to the cluster
ARRAY_ID=$(sbatch -a 1-$(cat $RUN_DIR/$JOBNAME.cmds | wc -l) --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/array.slurm | cut -f 4 -d " ")

# find failed jobs
find aligned_reads/ -size +1M -name "*.bam" | egrep -v "*.E*.dedup.rg.csorted.bam" | cut -f2 -d/ > wgrs-merge-bams.completed
# rerun failed jobs
JOBNAME="wgrs-fastp-bwa3"
NCORES=12
MEM=96
WALLTIME="8:00:00"
grep -vFf wgrs-merge-bams.completed wgrs-fastp-bwa.cmds > $JOBNAME.cmds
# submit to the cluster
ARRAY_ID=$(sbatch -a 1-$(cat $RUN_DIR/$JOBNAME.cmds | wc -l) --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/array.slurm | cut -f 4 -d " ")

NCORES=8
MEM=48
WALLTIME="2:00:00"
JOBNAME="wgrs-merge-bams2"

grep -vFf wgrs-merge-bams.completed wgrs-merge-bams.cmds > $JOBNAME.cmds
# submit to the cluster
ARRAY_ID=$(sbatch -a 1-$(cat $RUN_DIR/$JOBNAME.cmds | wc -l) --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/array.slurm | cut -f 4 -d " ")


```

### Variant calling
#### Calling variants (using Freebayes){#freebayes}

We used Freebayes *v1.3.5* [@garrisonHaplotypebasedVariantDetection2012] to assign variant probability scores and call variants. Notice that we used diploid mode (`-p 2`) and added the flag `--genotype-qualities` to be able to filter the resulting vcf file based on genotype qualities (`GQ`). 

```{bash call-variants-freebayes}
# setup workspace
CONDA_NAME="genomics" 
WORK_DIR="/scratch/project/adna/leela"
RUN_DIR="$WORK_DIR/FB_SNP_calling_10_10_2025"
GENOME="/scratch/project/adna/Papaya/Murdoch_sequencing/papaya_ref_grade/PAPHI003/hap2/PAPHI003_hap2"
# GENOME="/scratch/project/adna/Papaya/SunUp_reference_genome/GWHBFSC00000000"

# create a folder for all the bam files and standardise names
# BAM_DIR="$RUN_DIR/bam_files"
# mkdir -p $BAM_DIR
# find $RUN_DIR -name "*.bam*" -size +1M | egrep -v "\.E[0-9]+.dedup.rg.csorted.bam" | parallel --rpl  "{link} s:.+\/([^\.]+)\.(E\d+).+.bam:\1.bam:" "ln -sf {} $BAM_DIR/{link}"
# copy remaining bams (from repeat mapping)
# find $RUN_DIR/aligned_reads -name "*.bam*" -size +1M | egrep "\.E[0-9]+.dedup.rg.csorted.bam" | xargs -I {} ln -s {} $BAM_DIR
# rename -v 's/dedup.rg.csorted.bam/bam/' $BAM_DIR/*.bam*
# create a list of bam files
find $RUN_DIR -name "*.bam" -size +100M > $RUN_DIR/bam_list.txt

# index bams (if needed)
JOBNAME="index-bams"
NCORES=4
MEM=32
WALLTIME="2:00:00"
echo "find $BAM_DIR -name \"*.bam\" | parallel bamtools index -in {}" > $JOBNAME.cmds
# submit the job 
sbatch --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/parallel_jobs_run.slurm

cd $RUN_DIR


# Distributed freebayes (each node runs freebayes-parallel on one contig)
# download script
start_interactive_job
aria2c -c -x5 -d ~/bin https://raw.githubusercontent.com/freebayes/freebayes/master/scripts/split_ref_by_bai_datasize.py 
chmod +x ~/bin/split_ref_by_bai_datasize.py
# uncompress and genome
GENOME="/scratch/project/adna/Papaya/Murdoch_sequencing/papaya_ref_grade/PAPHI003/hap2/PAPHI003_hap2"
SIZE="1e9"
REGIONS_FILE="$RUN_DIR/Holland5_Hap2_target_regions_$SIZE.tsv"
gzip -cd $GENOME.fasta.gz > $GENOME.fasta
faidx $GENOME.fasta
# mamba install -y -n base numpy scipy

# fix library dependencies
# find $CONDA_PREFIX -name "libtabixpp.so*" | parallel ln -s {} {.}.0
# ln -s $CONDA_PREFIX/lib/libtabixpp.so.1 $CONDA_PREFIX/lib/libtabixpp.so.0
# split each contig/chromosome to smaller 1e6 bits
# prepare BAM files
JOBNAME="split_ref"
NCORES=4
MEM=32
WALLTIME="2:00:00"
# submit it as a Slurm job
# echo "~/bin/split_ref_by_bai_datasize.py -s 10e6 -r $GENOME.fasta.gz.fai $(find $BAM_DIR -name "*.bam" | egrep -v "\.E[0-9]+\." | xargs ls -1S | head -n1) > $RUN_DIR/Sunup_target_1e8_regions_chr.tsv" > $JOBNAME.cmds

echo "~/bin/split_ref_by_bai_datasize.py -s $SIZE -r $GENOME.fasta.fai -L $RUN_DIR/bam_list.txt > $REGIONS_FILE" > $JOBNAME.cmds
# submit the job 
sbatch --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/serial_jobs_run.slurm

JOBNAME="Murdoch_FB_diploid"
NCORES=12
MEM=32
WALLTIME="36:00:00"
# RUN_DIR=$WORK_DIR/SNP_calling_24_01_2025
PLOIDY=2
MIN_DP=7
# prepare commands
# BAM_FILES=$( find $BAM_DIR -name "*.bam" | egrep -v "\.E[0-9]+\." | xargs )
cut -f1 $GENOME.fasta.fai | parallel --dry-run "freebayes-parallel <(grep '{}' $REGIONS_FILE | gawk '{printf \"%s:%s-%s\n\", \$1, \$2, \$3}') \$SLURM_CPUS_PER_TASK -f $GENOME.fasta --genotype-qualities -g 200000 -C $MIN_DP -p $PLOIDY -L $RUN_DIR/bam_list.txt > $RUN_DIR/FB_array_output/{}.combined.vcf" > $RUN_DIR/$JOBNAME.cmds
mkdir -p $RUN_DIR/FB_array_output

# exit interactive job
# submit to the cluster
ARRAY_ID=$(sbatch -a 1-$(cat $RUN_DIR/$JOBNAME.cmds | wc -l) --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/array.slurm | cut -f 4 -d " ")

JOBNAME="freebayes-merge"
VCF_PREFIX="Cpapaya_Murdoch_WGRS_SRA_Holland5_Hap2.bwa2.fb.diploid"
echo "cat $RUN_DIR/FB_array_output/Chr*.combined.vcf | vcffirstheader | vcfstreamsort -w 1000 | vcfuniq > $VCF_PREFIX.vcf && bgzip $VCF_PREFIX.vcf && tabix $VCF_PREFIX.vcf.gz" > $JOBNAME.cmds
# submit job to cluster (or run as an interactive job)
sbatch --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/serial_jobs_run.slurm



```

 
#### Create and filter final variant file{#vcf-filter}
The variants that were called individually in each chromosome were merged into a single `vcf` file, which was gzipped and indexed. Variants were filtered using a combination of commands from SnpSift *v5.1d* [@rudenUsingDrosophilaMelanogaster2012], BCFtools *v1.17* [@danecekTwelveYearsSAMtools2021; @liStatisticalFrameworkSNP2011] and VCFtools *v0.1.16* [@danecekVariantCallFormat2011], based on their total loci depth and quality, keeping only bi-allelic polymorphic SNP loci with a depth of at least 10 and not more than 100,000 reads covering the locus and a minimum Quality of 30 (`10<DP<100000 & QUAL>30`, based on EDA). In addition, each samples's genotype call was reset (recoded as missing, or `./.`) if it had read depth (`DP<10`). Variant statistics were generated by BCFtools pre and post filter.

```{bash vcf_filter}
CONDA_NAME="genomics"
conda activate $CONDA_NAME
# Recode genotypes as missing if below a certain threshold, such as genotyping quality or depth (GQ:DP)  
# filter only polymorphic bi-allelic SNPs, using QUAL>20, 7<DP<100000

# filter Freebayes variants with SnpSift and vcftools (wipe any heterozygote genotype with DP<7 with bcftools)
QUAL=30 # 30
# MQ=30
MAX_DP=100000
MIN_DP=10
IND_DP=7
JOBNAME="Murdoch-wgrs-fb-filter"
VCF_PREFIX="Cpapaya_Murdoch_WGRS_SRA_Holland5_Hap2.bwa2.fb.diploid"

echo "bcftools filter -S . -e \"FMT/DP<$IND_DP\" $RUN_DIR/$VCF_PREFIX.vcf.gz -O v | SnpSift filter \"( QUAL>=$QUAL ) & (DP<$MAX_DP) & ( countRef()>=1 & countVariant()>=1 )\" | bgzip -@ \$SLURM_CPUS_PER_TASK -c > $RUN_DIR/$VCF_PREFIX.Q$QUAL.poly.recode.vcf.gz 
vcftools --gzvcf $RUN_DIR/$VCF_PREFIX.Q$QUAL.poly.recode.vcf.gz --recode --recode-INFO-all --minQ $QUAL --remove-indels -c | bgzip -@ \$SLURM_CPUS_PER_TASK -c > $RUN_DIR/$VCF_PREFIX.snps.Q$QUAL.poly.recode.vcf.gz 
vcftools --gzvcf $RUN_DIR/$VCF_PREFIX.Q$QUAL.poly.recode.vcf.gz --recode --recode-INFO-all --minQ $QUAL --keep-only-indels -c | bgzip -@ \$SLURM_CPUS_PER_TASK -c > $RUN_DIR/$VCF_PREFIX.indels.Q$QUAL.poly.recode.vcf.gz "> $RUN_DIR/$JOBNAME.cmds

NCORES=6
MEM=32
WALLTIME="1:00:00"
JOB_ID=$(sbatch --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/serial_jobs_run.slurm | cut -f 4 -d " ")

#bcftools filter -S . -e "GT=='het' | FMT/DP<$MIN_DP" A_rabiei_2024_Murdoch_WGRS_ArME14_v2.bwa2.fb.diploid.vcf.gz -O v | SnpSift filter "( QUAL>=$QUAL ) & ( DP<=$MAX_DP ) & ( DP>=$MIN_DP ) & ( countRef()>=1 & countVariant()>=1 )" | vcftools --vcf - --recode --recode-INFO-all --minQ $QUAL --max-missing 0.75 --remove-indels -c | bgzip -o A_rabiei_2024_Murdoch_WGRS_ArME14_v2.bwa2.fb.diploid.Q$QUAL.GT75.noRep.noHet.poly.recode.vcf.gz && tabix A_rabiei_2024_Murdoch_WGRS_ArME14_v2.bwa2.fb.diploid.Q$QUAL.GT75.noRep.noHet.poly.recode.vcf.gz
# generate stats 
JOBNAME="bcftools_stats"
WALLTIME=2:00:00
MEM=32
NCORES=8
find . -name "*.vcf.gz" | parallel --dry-run "bcftools stats -s - {} > {.}.bcfstats.txt" > $JOBNAME.cmds
# send to the cluster
sbatch --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/parallel_jobs_run.slurm 
```

#### Infer phylogeny from SNPs
##### Construction of phylogeny tree and population structure analysis

From the filtered SNPs the maximum likelihood phylogenetic tree of the 251 papaya accessions will be constructed using [SNPhylo](https://github.com/thlee/SNPhylo) [@leeSNPhyloPipelineConstruct2014].   
SNPhylo can construct phylogeny based on genome wide SNPs (vcf/hapmap format) by reduce SNP redundancy by linkage disequilibrium (LD) and produces maximum likelihood tree. 

> NOte that this pipeline is not maintained, consider using an alternative, such as [SNVphyl](https://snvphyl.readthedocs.io/en/latest/) [@petkauSNVPhylSingleNucleotide2017a] (see a comparison of pipelines in @katzComparativeAnalysisLyveSET2017).


```{bash snp-phylo}
#install SNPhylo and other packages associated to it.
# install in an interactive session on a compute node
start_debug_interactive_job
CONDA_NAME="genomics"
mamba create -n $CONDA_NAME snvphyl-tools



```



#### BCFtools stats
Generate stats from all variant files

```{bash bcftools-stats}

# generate stats 
JOBNAME="bcftools_stats"
WALLTIME=2:00:00
MEM=32
NCORES=8
CONDA_NAME="genomics"
find . -maxdepth 2 -name "*.vcf.gz" | parallel --dry-run --rpl "{base} s:.vcf.gz$::"  "bcftools stats -s - {} > {base}.bcfstats.txt" > $JOBNAME.cmds
# send to the cluster
sbatch --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/parallel_jobs_run.slurm 
```

#### MutilQC{#multiqc}
Quality metrics were collected from the raw read QC, alignment and variant calling steps and were consolidated into a single, interactive report for each batch using MultiQC v1.21 [@ewelsMultiQCSummarizeAnalysis2016]. 

```{bash multiqc}
NCORES=8
MEM=32
WALLTIME="10:00:00"
JOBNAME="Multiqc_Murdoch_WGRS"
# multiqc report
MULTIQC_JOB=QC_$(date +%d_%m_%Y)
# submit it as a Slurm job
echo "multiqc --interactive --force -i $MULTIQC_JOB -o $MULTIQC_JOB ." > $JOBNAME.cmds
# submit the job 
JOB_ID=$(sbatch --job-name=$JOBNAME --cpus-per-task=$NCORES --mem=${MEM}G --time=$WALLTIME --export=ALL,CMDS_FILE=$RUN_DIR/$JOBNAME.cmds,CONDA_NAME=$CONDA_NAME ~/bin/serial_jobs_run.slurm | cut -f 4 -d " " )
# Done!

# Copy html files to SharePoint
rclone copy -P --exclude "**.html" $RUN_DIR GRDC_rabiei:General/Projects/Hayley_PhD/Genetics/Murdoch_WGRS/SNP_calling_24_01_2025
# Copy html files to SharePoint
rclone copy -P --ignore-checksum --ignore-size --include "**.html" $RUN_DIR GRDC_rabiei:General/Projects/Hayley_PhD/Genetics/Murdoch_WGRS/SNP_calling_24_01_2025
```

------------------------------------------------------------------------

## General information {-}

This document was last updated on `r format(Sys.Date(), '%d %B %Y')` using R Markdown (built with `r R.version.string`). The source code for this website can be found on <https://github.com/IdoBar/SNP_Calling_Papaya>.

Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. It is especially powerful at authoring documents and reports which include code and can execute code and use the results in the output. For more details on using R Markdown see <http://rmarkdown.rstudio.com>, [R Markdown: The Definitive Guide](https://bookdown.org/yihui/rmarkdown/) and [Rmarkdown cheatsheet](https://rstudio.github.io/cheatsheets/html/rmarkdown.html).

## Bibliography {-}

